# LakehousePlumber Project Configuration
name: acme_edw
version: "1.0"
description: "acme Delta Lakehouse Project - TPC-H"
author: "Mehdi Modarressi"
created_date: "2025-07-11"

include:
  # - "01_raw_ingestion/csv_ingestions/customer_ingestion_incremental.yaml"
  - "01_raw_ingestion/**"
  # - "01_raw_ingestion/csv_ingestions/**"
  # - "01_raw_ingestion/json_ingestions/**"
  # - "01_raw_ingestion/parquet_ingestions/**"
  - "02_bronze/**"
  # - "03_silver/dim/**"
  # - "03_silver/fct/**"
  # - "04_gold/**"
  # - "09_test_python/test_py_func_flow.yaml"
  # - "03_silver/dim/customer_*.yaml"
  # - "02_bronze/customer_bronze.yaml"
  # - "02_bronze/customer_migration_bronze.yaml"
  # - "02_bronze/lineitem/**"
  # - "02_bronze/nation_bronze.yaml"
  # - "02_bronze/orders/**"
  # - "02_bronze/part_bronze.yaml"
  # - "02_bronze/partsupp_bronze.yaml"
  # - "02_bronze/region_bronze.yaml"
  # - "02_bronze/supplier_bronze.yaml"
  - "03_silver/**"
  # - "03_silver/fct/lineitem_*.yaml"
  # - "03_silver/dim/nation_*.yaml"
  # - "03_silver/fct/orders_*.yaml"
  # Test pipelines
  # - "05_tests/02_bronze_layer_tests.yaml"
  # - "03_silver/dim/region_*.yaml"
  # - "03_silver/dim/customer_*.yaml"
  # - "03_silver/dim/part_*.yaml"
  # - "03_silver/dim/partsupp_*.yaml"
  # - "03_silver/dim/region_*.yaml"
  # - "04_gold/customer_lifetime_value.yaml"
  # - "04_gold/customer_segmentation_mv.yaml"
  # - "04_gold/product_performance_mv.yaml"
  # - "04_Modelled/**"

  

operational_metadata:
  columns:
  
    _source_file_path:
      expression: "F.col('_metadata.file_path')"
      description: "File paths"
      applies_to: ["view"]
    
    _processing_timestamp:
      expression: "F.current_timestamp()"
      description: "When the record was processed by the pipeline"
      applies_to: ["streaming_table", "materialized_view", "view"]



    ##------------------------------------------------------
    ## Example of adding a cloudFiles metadata
    ## This is not used in the project, but is an example of how 
    ## to add a cloudFiles metadata
    ##------------------------------------------------------ 
    # _source_file_name:
    #   expression: "F.col('_metadata.file_name')"
    #   description: "Name of the input file along with its extension"
    #   applies_to: ["view"]

    # _source_file_size:
    #   expression: "F.col('_metadata.file_size')"
    #   description: "Length of the input file, in bytes"
    #   applies_to: ["view"]

    # _source_file_modification_time:
    #   expression: "F.col('_metadata.file_modification_time')"
    #   description: "Last modification time of the input file"
    #   applies_to: ["view"]

    ##------------------------------------------------------
    ## Example of adding a spark function for operational metadata
    ## This is not used in the project, but is an example of how 
    ## to add a spark function for operational metadata
    ##------------------------------------------------------ 
    # _record_hash:
    #   expression: "F.xxhash64(*[F.col(c) for c in df.columns])"
    #   description: "Hash of all record fields for change detection"
    #   applies_to: ["streaming_table", "materialized_view", "view"]
    #   additional_imports:
    #     - "from pyspark.sql.functions import hash"