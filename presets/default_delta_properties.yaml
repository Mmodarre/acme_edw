name: "default_delta_properties"
version: "1.0"
description: "Default preset with optimized Delta Lake properties and Spark configurations for data processing"

# Uncomment and customize the sections below as needed
defaults:
  # Load action defaults
  load_actions: {}
  # Example configurations - uncomment and modify as needed:
  # load_actions:
  #   cloudfiles:
  #     format: "parquet"
  #     schema_evolution_mode: "rescue"
  #     rescue_data_column: "_rescued_data"
  #     cloudfiles_use_notifications: false
  #     cloudfiles_validate_options: true
  #   
  #   delta:
  #     merge_schema: true
  #     optimize_write: true
  #     auto_compact: true
  #     checkpoint_interval: 10
  #   
  #   sql:
  #     spark_conf:
  #       "spark.sql.adaptive.enabled": "true"
  #       "spark.sql.adaptive.coalescePartitions.enabled": "true"
  #   
  #   python:
  #     enable_caching: true
  #     checkpoint_location_base: "/tmp/checkpoints"
  #   
  #   jdbc:
  #     batch_size: 10000
  #     isolation_level: "READ_COMMITTED"
  #     connection_timeout: 30

  # Transform action defaults  
  transform_actions: {}
  # Example configurations - uncomment and modify as needed:
  # transform_actions:
  #   sql:
  #     spark_conf:
  #       "spark.sql.adaptive.enabled": "true"
  #       "spark.sql.adaptive.skewJoin.enabled": "true"
  #       "spark.sql.adaptive.localShuffleReader.enabled": "true"
  #   
  #   python:
  #     enable_arrow_based_columnar_data_transfers: true
  #     spark_conf:
  #       "spark.sql.execution.arrow.pyspark.enabled": "true"
  #   
  #   data_quality:
  #     on_violation: "fail"
  #     enable_quarantine: true
  #     quarantine_rules:
  #       - "invalid_data"
  #       - "constraint_violations"
  #   
  #   temp_table:
  #     cache_table: true
  #     storage_level: "MEMORY_AND_DISK_SER"
  #   
  #   schema:
  #     enforce_schema: true
  #     allow_schema_evolution: false

  # Write action defaults
  # write_actions: {}
  # Example configurations - uncomment and modify as needed:
  write_actions:
    streaming_table:
      table_properties:
        delta.enableRowTracking: "true"
        # delta.autoOptimize.optimizeWrite: true
        # delta.autoOptimize.autoCompact: true
        # delta.enableChangeDataFeed: true
        # delta.columnMapping.mode: name
        # delta.feature.allowColumnDefaults: supported
        # delta.tuneFileSizesForRewrites: true
  #       "delta.autoOptimize.optimizeWrite": "true"
  #       "delta.autoOptimize.autoCompact": "true"
  #       "delta.enableChangeDataFeed": "true"
  #       "delta.columnMapping.mode": "name"
  #       "delta.feature.allowColumnDefaults": "supported"
  #       "delta.tuneFileSizesForRewrites": "true"
  #     spark_conf:
  #       "spark.sql.adaptive.enabled": "true"
  #       "spark.sql.adaptive.coalescePartitions.enabled": "true"
  #       "spark.databricks.delta.optimizeWrite.enabled": "true"
  #       "spark.databricks.delta.autoCompact.enabled": "true"
  #   
  #   materialized_view:
  #     table_properties:
  #       "delta.autoOptimize.optimizeWrite": "true"
  #       "delta.autoOptimize.autoCompact": "true"
  #       "delta.enableChangeDataFeed": "false"
  #       "delta.columnMapping.mode": "name"
  #       "delta.feature.allowColumnDefaults": "supported"
  #     spark_conf:
  #       "spark.sql.adaptive.enabled": "true"
  #       "spark.sql.adaptive.coalescePartitions.enabled": "true"
  #       "spark.sql.adaptive.skewJoin.enabled": "true"

  # Operational metadata defaults - uncomment and modify as needed:
  # operational_metadata:
  #   enabled: true
  #   preset: "standard"
  #   columns:
  #     - "_lakehouse_timestamp"
  #     - "_lakehouse_source_file"
  #     - "_lakehouse_ingestion_date"
  #     - "_lakehouse_processing_date"
  #     - "_lakehouse_batch_id"
