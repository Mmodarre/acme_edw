# This is a template for ingesting CSV files with schema enforcement
# It is used to generate the actions for the pipeline
# within the pipeline all it need to defined are the parameters for the table name and landing folder
# the template will generate the actions for the pipeline

name: csv_ingestion_template
version: "1.0"
description: "Standard template for ingesting CSV files with schema enforcement"


parameters:
  - name: table_name
    required: true
    description: "Name of the table to ingest"
  - name: landing_folder
    required: true
    description: "Name of the landing folder"
  - name: table_properties
    required: false
    description: "Optional table properties as key-value pairs"
    default: {}
  - name: cluster_columns
    required: false
    description: "Optional Liquid clustering columns"
    default: []
  - name: schema_file
    required: true
    description: "Schema file name"
    default: ""

actions:
  - name: load_{{ table_name }}_csv
    type: load
    readMode : "stream"
    # Natural YAML array format (was JSON string)
    operational_metadata:
      - "_source_file_path"
      - "_processing_timestamp"
    source:
      type: cloudfiles
      path: "{landing_volume}/{{ landing_folder }}/*.csv"
      format: csv
      # Natural YAML object format (was JSON string)
      options:
        cloudFiles.format: csv
        header: "True"
        delimiter: ","
        cloudFiles.maxFilesPerTrigger: "50"
        cloudFiles.inferColumnTypes: False
        cloudFiles.schemaEvolutionMode: addNewColumns
        cloudFiles.rescuedDataColumn: _rescued_data
        # Managed file events is not available in Databricks Free Edition, Uncomment to use it if you are not using the free edition
        # cloudFiles.useManagedFileEvents: True  
        cloudFiles.schemaHints: "schemas/{{ schema_file }}.yaml"

    target: v_{{ table_name }}_cloudfiles
    description: "Load {{ table_name }} CSV files from landing volume"

  - name: write_{{ table_name }}_cloudfiles
    type: write
    source: v_{{ table_name }}_cloudfiles
    write_target:
      type: streaming_table
      database: "{catalog}.{raw_schema}"
      table: "{{ table_name }}"
      cluster_columns: "{{ cluster_columns }}"
      description: "Write {{ table_name }} to raw layer" 
      table_properties: "{{ table_properties }}"